# Amazon Athena Google BigQuery connector<a name="connectors-bigquery"></a>

The Amazon Athena connector for Google [BigQuery](https://cloud.google.com/bigquery/) enables Amazon Athena to run SQL queries on your Google BigQuery data\.

## Prerequisites<a name="connectors-bigquery-prerequisites"></a>
+ Deploy the connector to your AWS account using the Athena console or the AWS Serverless Application Repository\. For more information, see [Deploying a data source connector](connect-to-a-data-source-lambda.md) or [Using the AWS Serverless Application Repository to deploy a data source connector](connect-data-source-serverless-app-repo.md)\.
+ Set up a VPC and a security group before you use this connector\. For more information, see [Creating a VPC for a data source connector](athena-connectors-vpc-creation.md)\.

## Limitations<a name="connectors-bigquery-limitations"></a>
+ Lambda functions have a maximum timeout value of 15 minutes\. Each split executes a query on BigQuery and must finish with enough time to store the results for Athena to read\. If the Lambda function times out, the query fails\.
+ Google BigQuery is case sensitive\. The connector attempts to correct the case of dataset names and table names but does not do any case correction for project IDs\. This is necessary because Athena lower cases all metadata\. These corrections make many extra calls to Google BigQuery\.
+ Binary data types are not supported\.
+ Complex data types such as `map`, `list`, and `struct` are not supported\.
+ Because of Google BigQuery concurrency and quota limits, the connector may encounter Google quota limit issues\. To avoid these issues, push as many constraints to Google BigQuery as feasible\. For information about BigQuery quotas, see [Quotas and limits](https://cloud.google.com/bigquery/quotas) in the Google BigQuery documentation\.

## Parameters<a name="connectors-bigquery-parameters"></a>

Use the Lambda environment variables in this section to configure the Google BigQuery connector\.
+ **spill\_bucket** – Specifies the Amazon S3 bucket for data that exceeds Lambda function limits\.
+ **spill\_prefix** – \(Optional\) Defaults to a subfolder in the specified `spill_bucket` called `athena-federation-spill`\. We recommend that you configure an Amazon S3 [storage lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html) on this location to delete spills older than a predetermined number of days or hours\.
+ **spill\_put\_request\_headers** – \(Optional\) A JSON encoded map of request headers and values for the Amazon S3 `putObject` request that is used for spilling \(for example, `{"x-amz-server-side-encryption" : "AES256"}`\)\. For other possible headers, see [PutObject](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html) in the *Amazon Simple Storage Service API Reference*\.
+ **kms\_key\_id** – \(Optional\) By default, any data that is spilled to Amazon S3 is encrypted using the AES\-GCM authenticated encryption mode and a randomly generated key\. To have your Lambda function use stronger encryption keys generated by KMS like `a7e63k4b-8loc-40db-a2a1-4d0en2cd8331`, you can specify a KMS key ID\.
+ **disable\_spill\_encryption** – \(Optional\) When set to `True`, disables spill encryption\. Defaults to `False` so that data that is spilled to S3 is encrypted using AES\-GCM – either using a randomly generated key or KMS to generate keys\. Disabling spill encryption can improve performance, especially if your spill location uses [server\-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)\.
+ **gcp\_project\_id** – The project ID \(not project name\) that contains the datasets that the connector should read from \(for example, `semiotic-primer-1234567`\)\.
+ **secret\_manager\_gcp\_creds\_name** – The name of the secret within AWS Secrets Manager that contains your BigQuery credentials in JSON format \(for example, `GoogleCloudPlatformCredentials`\)\.

## Partitions and splits<a name="connectors-bigquery-partitions-and-splits"></a>

The BigQuery connector uses the `concurrencyLimit` environment variable that is defined in your Google project to determine the page count for the splits\. The connector does not generate splits as a function of the partitions over the table\.

## Performance<a name="connectors-bigquery-performance"></a>

The Lambda function performs predicate pushdown to decrease the data scanned by the query\. `LIMIT` clauses reduce the amount of data scanned, but if you do not provide a predicate, you should expect `SELECT` queries with a `LIMIT` clause to scan at least 16 MB of data\. Selecting a subset of columns significantly speeds up query runtime and reduces data scanned\. The connector is subject to query failures as concurrency increases, and generally is a slow connector\.

## License information<a name="connectors-bigquery-license-information"></a>

The Amazon Athena Google BigQuery connector project is licensed under the [Apache\-2\.0 License](https://www.apache.org/licenses/LICENSE-2.0.html)\.

By using this connector, you acknowledge the inclusion of third party components, a list of which can be found in the [pom\.xml](https://github.com/awslabs/aws-athena-query-federation/blob/master/athena-google-bigquery/pom.xml) file for this connector, and agree to the terms in the respective third party licenses provided in the [LICENSE\.txt](https://github.com/awslabs/aws-athena-query-federation/blob/master/athena-google-bigquery/LICENSE.txt) file on GitHub\.com\.

## See also<a name="connectors-bigquery-see-also"></a>

For additional information about this connector, visit [the corresponding site](https://github.com/awslabs/aws-athena-query-federation/tree/master/athena-google-bigquery) on GitHub\.com\.